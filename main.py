import sys
import os
import random
import numpy as np
import cv2
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import pygame
import time

from envs.ski_env import make_skiing_env
from utils.human_player import HumanPlayer
def process_img(image):
    """å¤„ç†å›¾åƒä¸ºçš„äºŒå€¼å›¾"""
    if isinstance(image, tuple):  # å¦‚æœæ˜¯(obs, info)å…ƒç»„
        image = image[0]
    image = cv2.resize(image, (128, 128))
    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)  # ä¿®æ”¹ä¸ºRGBè½¬ç°åº¦
    _, image = cv2.threshold(image, 230, 255, cv2.THRESH_BINARY)
    # image = image / 255.0  # å½’ä¸€åŒ–åˆ°0-1
    # æ˜¾ç¤ºimage
    cv2.imshow('Processed Frame', image)
    # cv2.imwrite('./results/processed_frame.png', image)  # ä¿å­˜ä¸ºå›¾åƒæ–‡ä»¶
    return image

class params():
    def __init__(self):
        self.gamma = 0.99
        self.action_dim = 3  # 3ç§åŠ¨ä½œ
        self.obs_dim = (4, 128, 128)  # 4å¸§å †å 
        self.capacity = 10000  # å¢å¤§ç»éªŒæ± å®¹é‡
        self.cuda = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        self.Frames = 4
        self.episodes = int(1e2)  # å‡å°‘æ€»å¹•æ•°è¿›è¡Œæµ‹è¯•
        self.updatebatch = 512  # å¢å¤§æ‰¹æ¬¡å¤§å°
        self.test_episodes = 10  # å‡å°‘æµ‹è¯•å¹•æ•°
        self.epsilon = 0.001  # åˆå§‹æ¢ç´¢ç‡
        self.epsilon_min = 0.0001  # æœ€å°æ¢ç´¢ç‡
        self.epsilon_decay = 0.995  # æ¢ç´¢ç‡è¡°å‡
        self.Q_NETWORK_ITERATION = 100  # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
        self.learning_rate = 0.0001  # è°ƒæ•´å­¦ä¹ ç‡
        self.agent_type = "DQN"  # åˆå§‹åŒ–

def create_agent(env, args, agent_type="DQN"):
    # ä½¿ç”¨ args ä¸­å®šä¹‰çš„ obs_dimï¼ˆå› ä¸ºæ˜¯å›¾åƒå †å ï¼‰
    act_dim = args.action_dim # è¿™ä¸ªä»ç„¶éœ€è¦ï¼Œä½†DQNå†…éƒ¨ä¹Ÿä¼šä»argä¸­è·å–
    if agent_type == "DQN":
        from agents.DQN_agent import DQN
        agent = DQN(env, args) # ä¿®æ­£ï¼šDQN çš„ __init__ ç­¾åæ˜¯ (self, env, arg)
    elif agent_type == "NoisyDQN":
        from agents.noisydqn_agent import NoisyDQN
        agent = NoisyDQN(env, args) 

    else:
        raise ValueError(f"æœªçŸ¥çš„ Agent ç±»å‹: {agent_type}")
    return agent

def load_state(path, agent):
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
    if os.path.exists(path):
        state_dict = torch.load(path, map_location=agent.arg.cuda)
        agent.Net.load_state_dict(state_dict)
        agent.targetNet.load_state_dict(state_dict)
        print(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {path}")
    else:
        print(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {path}")

def _process_frame(frame):
    """å¤„ç†å•å¸§å›¾åƒ"""
    if isinstance(frame, tuple):  # å¦‚æœæ˜¯resetè¿”å›çš„å…ƒç»„
        frame = frame[0] if isinstance(frame[0], np.ndarray) else frame
    return process_img(frame)

def _make_init_obs(raw_frame):
    """åˆ›å»ºåˆå§‹è§‚æµ‹ï¼ˆ4å¸§å †å ï¼‰"""
    frame = _process_frame(raw_frame)
    return np.stack([frame] * 4, axis=0)  # shape=(4,H,W)

def _roll_obs(prev_obs, new_frame):
    """æ»šåŠ¨æ›´æ–°è§‚æµ‹å¸§"""
    new_frame = _process_frame(new_frame)[np.newaxis]  # (1,H,W)
    return np.concatenate([new_frame, prev_obs[:-1]], axis=0)

def training(arg, agent, env, save_path, final_path, reward_curve_path):
    """è®­ç»ƒå‡½æ•°ï¼ˆæ”¯æŒè‡ªå®šä¹‰ä¿å­˜è·¯å¾„ï¼‰"""
    reward_curve = []
    fig, ax = plt.subplots(figsize=(10, 6))

    best_reward = -float('inf')

    for episode in range(arg.episodes):
        print(f"Starting episode {episode}")

        # é‡ç½®ç¯å¢ƒ
        # print(env._use_images)
        # reset_result = env.reset(options={"use_images": False})
        # print(reset_result[0])
        # print(env._use_images)
        reset_result = env.reset()

        if isinstance(reset_result, tuple):
            raw_frame, info = reset_result
        else:
            raw_frame, info = reset_result, {}

        # åˆå§‹åŒ–è§‚æµ‹å€¼
        obs = _make_init_obs(raw_frame)
        done = False
        total_reward = 0
        step_count = 0

        traj = dict(obs=[], action=[], reward=[], next_obs=[], done=[])

        # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
        while not done:
            if random.random() < arg.epsilon:
                action = random.randint(0, arg.action_dim - 1)
                # print("éšæœºé€‰æ‹©:", action)
            else:
                action = agent.get_action(obs)
                # print("è´ªå¿ƒé€‰æ‹©:", action)

            step_result = env.step(action)

            if len(step_result) == 5:
                next_frame, reward, terminated, truncated, info = step_result
                done = terminated or truncated
            else:
                next_frame, reward, done, info = step_result

            if done:
                reward -= 100  # ç»ˆæ­¢æƒ©ç½š

            total_reward += reward
            step_count += 1

            next_obs = _roll_obs(obs, next_frame)

            traj['obs'].append(obs.copy())
            traj['action'].append(action)
            traj['reward'].append(reward)
            traj['next_obs'].append(next_obs.copy())
            traj['done'].append(done)

            obs = next_obs

            if step_count > 10000:
                break

        if len(traj['obs']) > 0:
            agent.Buffer.store_data(traj, len(traj['obs']))

        arg.epsilon = max(arg.epsilon_min, arg.epsilon * arg.epsilon_decay)

        # å¦‚æœ buffer å¤§å°è¶…è¿‡ 1000ï¼Œåˆ™è¿›è¡Œæ›´æ–°
        if hasattr(agent.Buffer, 'ptr') and agent.Buffer.ptr > 1000:
            for _ in range(10):
                batch = agent.Buffer.sample(arg.updatebatch)
                if batch is not None:
                    loss = agent.update(batch)
                    if loss is not None and episode % 100 == 0:
                        print(f"Episode {episode}, Loss: {loss:.4f}")

        # æ¯ 50 è½®è¿›è¡Œä¸€æ¬¡æ€§èƒ½æµ‹è¯•å’Œæ¨¡å‹ä¿å­˜
        if episode % 50 == 0:
            original_render_mode = env.render_mode
            env.close()
            test_env = make_skiing_env("Skiing-rgb-v0", render_mode="rgb_array")  # æµ‹è¯•æ—¶ä¸æ¸²æŸ“çª—å£

            avg_reward = test_performance(arg, agent, test_env, model_path=None)  # å·²åŠ è½½
            reward_curve.append(avg_reward)

            print(f'Episode {episode:>6} | Test Reward: {avg_reward:.2f} | '
                  f'Epsilon: {arg.epsilon:.3f} | Steps: {step_count}')

            if avg_reward > best_reward:
                best_reward = avg_reward
                torch.save(agent.Net.state_dict(), save_path)
                print(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜! å¥–åŠ±: {best_reward:.2f}")

            test_env.close()
            env = make_skiing_env("Skiing-rgb-v0", render_mode=original_render_mode)

            ax.clear()
            ax.plot(reward_curve, 'b-', linewidth=2)
            ax.set_xlabel('Episode (x50)')
            ax.set_ylabel('Average Reward')
            ax.set_title(f'Training Progress - {arg.agent_type}')
            ax.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(reward_curve_path, dpi=150, bbox_inches='tight')

    torch.save(agent.Net.state_dict(), final_path)
    print(f"âœ… è®­ç»ƒå®Œæˆ! æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_path}")

def test_performance(arg, agent, test_env=None, model_path=None):
    """æµ‹è¯•æ€§èƒ½ï¼ˆæ”¯æŒåŠ è½½æŒ‡å®šæ¨¡å‹ï¼‰"""
    if model_path:
        load_state(model_path, agent)

    if test_env is None:
        test_env = make_skiing_env("Skiing-rgb-v0", render_mode=None)

    rewards = []
    for _ in range(arg.test_episodes):
        reset_result = test_env.reset()
        if isinstance(reset_result, tuple):
            raw_frame, info = reset_result
        else:
            raw_frame, info = reset_result, {}
        obs = _make_init_obs(raw_frame)
        done = False
        ep_reward = 0
        step_count = 0

        while not done:
            action = agent.greedy_action(obs)
            step_result = test_env.step(action)
            if len(step_result) == 5:
                _, reward, terminated, truncated, _ = step_result
                done = terminated or truncated
            else:
                _, reward, done, _ = step_result
            obs = _roll_obs(obs, step_result[0])
            ep_reward += reward
            step_count += 1
            if step_count > 1000:
                break
        rewards.append(ep_reward)

    if test_env:
        test_env.close()

    return np.mean(rewards)

def demo_play(arg, agent, env, model_path=None):
    """æ¼”ç¤ºæ¨¡å¼ï¼ˆå¸¦çª—å£ï¼‰"""
    if model_path:
        load_state(model_path, agent)  # ä¼ å…¥è·¯å¾„
    arg.epsilon = 0.0

    reset_result = env.reset()
    if isinstance(reset_result, tuple):
        raw_frame, info = reset_result
    else:
        raw_frame, info = reset_result, {}
    obs = _make_init_obs(raw_frame)
    done = False
    total_reward = 0
    step_count = 0

    print("ğŸ® å¼€å§‹æ¼”ç¤º! æŒ‰ ESC é€€å‡ºï¼ŒR é‡æ–°å¼€å§‹")

    try:
        while True:
            for event in pygame.event.get():
                if event.type == pygame.QUIT or (event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE):
                    return
                elif event.type == pygame.KEYDOWN and event.key == pygame.K_r and done:
                    reset_result = env.reset()
                    if isinstance(reset_result, tuple):
                        raw_frame, info = reset_result
                    else:
                        raw_frame, info = reset_result, {}
                    obs = _make_init_obs(raw_frame)
                    done = False
                    total_reward = 0
                    step_count = 0
                elif event.type == pygame.MOUSEBUTTONDOWN and done:
                    mouse_pos = event.pos
                    restart_button = pygame.Rect(env._screen_width // 2 - 100, env._screen_height // 2 + 30, 200, 50)
                    quit_button = pygame.Rect(env._screen_width // 2 - 100, env._screen_height // 2 + 100, 200, 50)
                    if restart_button.collidepoint(mouse_pos):
                        reset_result = env.reset()
                        if isinstance(reset_result, tuple):
                            raw_frame, info = reset_result
                        else:
                            raw_frame, info = reset_result, {}
                        obs = _make_init_obs(raw_frame)
                        done = False
                        total_reward = 0
                        step_count = 0
                    elif quit_button.collidepoint(mouse_pos):
                        return

            if not done:
                action = agent.greedy_action(obs)
                step_result = env.step(action)
                if len(step_result) == 5:
                    next_frame, reward, terminated, truncated, info = step_result
                    done = terminated or truncated
                else:
                    next_frame, reward, done, info = step_result
                obs = _roll_obs(obs, next_frame)
                total_reward += reward
                step_count += 1

                if step_count % 50 == 0:
                    print(f"æ­¥æ•°: {step_count}, ç´¯è®¡å¥–åŠ±: {total_reward:.1f}")

                time.sleep(0.01)

                if done:
                    final_score = info.get("score", total_reward)
                    print(f"ğŸ¯ æ¼”ç¤ºç»“æŸ! æ€»æ­¥æ•°: {step_count}, æœ€ç»ˆå¾—åˆ†: {final_score}")
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­! æœ€ç»ˆå¾—åˆ†: {total_reward}")
def human_play_mode(env):
    """äººå·¥ç©æ³•æ¨¡å¼"""
    # åˆ›å»ºäººå·¥ç©å®¶
    human_player = HumanPlayer()
    
    # é‡ç½®ç¯å¢ƒ
    reset_result = env.reset()
    if isinstance(reset_result, tuple):
        raw_frame, info = reset_result
    else:
        raw_frame, info = reset_result, {}
    
    done = False
    total_reward = 0
    step_count = 0
    
    try:
        while True:
            # å¤„ç†é€€å‡ºäº‹ä»¶
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    return
                elif event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_ESCAPE:
                        print("æ¸¸æˆé€€å‡º")
                        return
                    elif event.key == pygame.K_r and done:
                        # é‡æ–°å¼€å§‹æ¸¸æˆ
                        reset_result = env.reset()
                        if isinstance(reset_result, tuple):
                            raw_frame, info = reset_result
                        else:
                            raw_frame, info = reset_result, {}
                        human_player.reset()
                        done = False
                        total_reward = 0
                        step_count = 0
                        print("æ¸¸æˆé‡æ–°å¼€å§‹!")
                # å¤„ç†é¼ æ ‡ç‚¹å‡»ï¼ˆç”¨äºæ¸¸æˆç»“æŸç•Œé¢çš„æŒ‰é’®ï¼‰
                elif event.type == pygame.MOUSEBUTTONDOWN and done:
                    mouse_pos = event.pos
                    restart_button = pygame.Rect(env._screen_width // 2 - 100, env._screen_height // 2 + 30, 200, 50)
                    quit_button = pygame.Rect(env._screen_width // 2 - 100, env._screen_height // 2 + 100, 200, 50)
                    if restart_button.collidepoint(mouse_pos):
                        # è§¦å‘é‡æ–°å¼€å§‹
                        reset_result = env.reset()
                        if isinstance(reset_result, tuple):
                            raw_frame, info = reset_result
                        else:
                            raw_frame, info = reset_result, {}
                        human_player.reset()
                        done = False
                        total_reward = 0
                        step_count = 0
                        print("æ¸¸æˆé‡æ–°å¼€å§‹!")
                    elif quit_button.collidepoint(mouse_pos):
                        # è§¦å‘é€€å‡ºæ¸¸æˆ
                        print("æ¸¸æˆé€€å‡º")
                        return
            
            if not done:
                # è·å–äººå·¥ç©å®¶åŠ¨ä½œ
                action = human_player.get_action_from_keyboard()
                
                # æ‰§è¡ŒåŠ¨ä½œ
                step_result = env.step(action)
                if len(step_result) == 5:
                    next_frame, reward, terminated, truncated, info = step_result
                    done = terminated or truncated
                else:
                    next_frame, reward, done, info = step_result
                
                total_reward += reward
                step_count += 1
                
                # æ˜¾ç¤ºå®æ—¶ä¿¡æ¯
                if step_count % 50 == 0:
                    print(f"å¾—åˆ†: {info.get('score', 0):.1f}, é€Ÿåº¦: {info.get('speed', 0):.1f}, æ——å¸œæ•°é‡: {info.get('flag_count', 0)}")
                
                # é™åˆ¶æœ€å¤§æ­¥æ•°
                if step_count > 5000:
                    done = True
                    # è®¾ç½®æ¸¸æˆç»“æŸçŠ¶æ€
                    if hasattr(env, '_game_over'):
                        env._game_over = True
                
            # æ¸²æŸ“æ¸¸æˆ
            env.render()
            time.sleep(0.05 if not done else 0.01)  # æ§åˆ¶æ¸¸æˆé€Ÿåº¦
            
    except KeyboardInterrupt:
        print("æ¸¸æˆé€€å‡º")
    finally:
        env.close()

def test_with_display(arg, agent, model_path=None):
    """å¸¦çª—å£çš„æµ‹è¯•æ¨¡å¼"""
    load_state(model_path, agent)
    test_env = make_skiing_env("Skiing-rgb-v0", render_mode="human", debug=True)
    rewards = []

    for episode in range(arg.test_episodes):
        reset_result = test_env.reset()
        if isinstance(reset_result, tuple):
            raw_frame, info = reset_result
        else:
            raw_frame, info = reset_result, {}
        obs = _make_init_obs(raw_frame)
        done = False
        ep_reward = 0
        step_count = 0

        print(f"â–¶ï¸ å¼€å§‹æµ‹è¯•ç¬¬ {episode + 1}/{arg.test_episodes} å¹•...")

        while not done:
            action = agent.greedy_action(obs)
            step_result = test_env.step(action)
            if len(step_result) == 5:
                next_frame, reward, terminated, truncated, info = step_result
                done = terminated or truncated
            else:
                next_frame, reward, done, info = step_result
            obs = _roll_obs(obs, next_frame)
            ep_reward += reward
            step_count += 1
            time.sleep(0.03)
            if step_count > 500:
                break

        rewards.append(ep_reward)
        print(f"ğŸ”š ç¬¬ {episode + 1} å¹•ç»“æŸ: å¥–åŠ±={ep_reward:.1f}, æ­¥æ•°={step_count}")
        if episode < arg.test_episodes - 1:
            time.sleep(2)

    test_env.close()
    avg_reward = np.mean(rewards)
    print(f"\nğŸ“Š æµ‹è¯•å®Œæˆ! å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    return avg_reward

if __name__ == '__main__':
    # ==================== é…ç½®åŒºåŸŸ====================
    AGENT_TYPE = "DQN"  # å¯é€‰: "DQN", "NoisyDQN"
    MODEL_SAVE_PATH = f"models/ski_{AGENT_TYPE.lower()}_best_flag.pkl"
    MODEL_FINAL_PATH = f"models/ski_{AGENT_TYPE.lower()}_final.pkl"
    REWARD_CURVE_PATH = f"results/reward_curve_{AGENT_TYPE.lower()}.jpg"

    os.makedirs("models", exist_ok=True)
    os.makedirs("results", exist_ok=True)

    arg = params()
    arg.agent_type = AGENT_TYPE

    mode = input("é€‰æ‹©æ¨¡å¼ (1-è®­ç»ƒ, 2-æµ‹è¯•, 3-æ¼”ç¤º, 4-æ¸¸ç©): ").strip()

    # try:
    if mode == "1":  # è®­ç»ƒ
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {AGENT_TYPE}...")
        # env = make_skiing_env("Skiing-rgb-v0", render_mode="rgb_array")  # æ— çª—å£æ¸²æŸ“ï¼Œä½¿ç”¨å›¾åƒ
        env = make_skiing_env("Skiing-rgb-v0", render_mode="human")  # æœ‰çª—å£æ¸²æŸ“ï¼Œä¾¿äºè°ƒè¯•
        agent = create_agent(env, arg, AGENT_TYPE)
        load_state(MODEL_SAVE_PATH, agent)
        training(arg, agent, env, MODEL_SAVE_PATH, MODEL_FINAL_PATH, REWARD_CURVE_PATH)

    elif mode == "2":  # æµ‹è¯•ï¼ˆå¸¦æ˜¾ç¤ºï¼‰
        print(f"ğŸ§ª å¼€å§‹æµ‹è¯• {AGENT_TYPE}ï¼ˆå¸¦çª—å£ï¼‰...")
        env = make_skiing_env("Skiing-rgb-v0", render_mode="human", debug=True)
        agent = create_agent(env, arg, AGENT_TYPE)
        test_with_display(arg, agent, MODEL_SAVE_PATH)

    elif mode == "3":  # æ¼”ç¤º
        print(f"ğŸ¬ å¼€å§‹æ¼”ç¤º {AGENT_TYPE}...")
        env = make_skiing_env("Skiing-rgb-v0", render_mode="human", debug=True)
        agent = create_agent(env, arg, AGENT_TYPE)
        demo_play(arg, agent, env, MODEL_SAVE_PATH)

    elif mode == "4":  # äººå·¥æ¸¸ç©
        print("ğŸ® äººå·¥ç©å®¶æ¨¡å¼...")
        env = make_skiing_env("Skiing-rgb-v0", render_mode="human", debug=True)
        human_play_mode(env)

    else:
        print("âš ï¸ æ— æ•ˆè¾“å…¥ï¼Œå¯åŠ¨è®­ç»ƒæ¨¡å¼...")
        env = make_skiing_env("Skiing-rgb-v0", render_mode="rgb_array")  # æ— çª—å£æ¸²æŸ“ï¼Œä½¿ç”¨å›¾åƒ
        # env = make_skiing_env("Skiing-rgb-v0", render_mode="human")  # æœ‰çª—å£æ¸²æŸ“ï¼Œä¾¿äºè°ƒè¯•
        agent = create_agent(env, arg, AGENT_TYPE)
        load_state(MODEL_SAVE_PATH, agent)
        training(arg, agent, env, MODEL_SAVE_PATH, MODEL_FINAL_PATH, REWARD_CURVE_PATH)

    env.close()
    # except Exception as e:
    #     print(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
    # finally:
    #     pygame.quit()
    #     cv2.destroyAllWindows()